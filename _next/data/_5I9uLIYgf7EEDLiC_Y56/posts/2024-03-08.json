{"pageProps":{"prev":{"meta":{"title":"Markov Decision Process","desc":"Markov Decision Process","date":"2024.03.06","tags":["AI","machine learning","Reinforce ment learning","Markov Decision Process","MDP"]},"content":"\n\n\n\n\n\n### Markov Decision Process (MDP)의 정의\n\n**마르코프 결정 과정(Markov Decision Process, MDP)** 은 강화학습의 수학적 모델 중 하나로, 순차적 의사결정 문제를 다루는 데 사용됩니다. MDP는 **상태(states)**, **행동(actions)**, **보상 함수(rewards)**, 그리고 **상태 전이 확률(transitions)** 이라는 네 가지 주요 구성 요소로 이루어져 있습니다.\n\n### MDP의 구성 요소\n\n**상태(S)**: 에이전트가 처할 수 있는 모든 가능한 환경의 상황을 의미합니다.\n**행동(A)**: 에이전트가 각 상태에서 취할 수 있는 모든 가능한 행동을 나타냅니다.\n**보상 함수(R)**: 특정 상태에서 특정 행동을 했을 때 에이전트가 받게 되는 보상을 정의합니다. 이 보상은 에이전트가 얼마나 \"잘\" 행동했는지를 수치적으로 평가하는 지표입니다.\n**상태 전이 확률(P)**: 어떤 상태에서 특정 행동을 한 후 다른 상태로 이동할 확률을 나타냅니다. 이는 환경의 동적인 특성을 모델링하는 데 중요합니다.\n\n### MDP의 특징\n\n**마르코프 성질**: MDP의 기본 가정 중 하나는 마르코프 성질입니다. 이는 미래의 상태가 오직 현재의 상태와 행동에만 의존하며, 과거의 상태나 행동에는 의존하지 않는다는 성질을 의미합니다.\n**결정론적 및 확률적 행동**: MDP에서는 행동이 결정론적이거나 확률적일 수 있습니다. 결정론적인 경우, 행동 결과는 항상 같은 상태로 이어지며, 확률적인 경우 다양한 가능성이 존재합니다.\n\n### MDP의 적용\n\nMDP는 다양한 실세계 문제에 적용할 수 있습니다. 예를 들어, 자율 주행 자동차, 로봇 팔 조작, 경제 정책 결정, 게임 플레이 등 복잡한 결정을 요구하는 다양한 분야에서 사용됩니다. 이러한 문제들을 모델링하고 최적의 결정 규칙(정책)을 학습하는 것이 MDP를 사용하는 주된 목적입니다.\n\n\n\n### MDP의 목표\nMDP는 강화학습에서 중요한 이론적 기반을 제공하며, 복잡한 의사결정 문제를 효과적으로 해결하기 위한 강력한 도구입니다. 이 모델을 통해 에이전트는 최적의 행동 전략을 배울 수 있으며, 이는 궁극적으로 **주어진 환경에서 최대의 보상**을 얻기 위한 방법을 찾는 데 도움을 줍니다.\n마르코프 결정 과정(Markov Decision Process, MDP)의 주요 목표는 기대되는 반환값(보상의 합)을 최대화하는 것입니다. 이 목표를 달성하기 위해 에이전트는 상태와 행동 간의 상호작용을 통해 최적의 정책(policy)를 찾아내야 합니다. 최적의 정책은 주어진 상태에서 에이전트가 선택할 수 있는 행동들 중에서 기대 반환값을 최대화하는 행동을 선택하도록 합니다.\n\nMDP에서 반환값은 일련의 행동을 통해 얻은 보상의 미래 가치를 고려한 총합입니다. 이 보상은 종종 감마(γ)라는 할인 계수를 사용하여 할인되며, 이는 미래의 보상이 현재 가치에 미치는 영향을 조정합니다. 할인 계수는 보상을 받는 시점의 지연에 따른 가치의 감소를 반영합니다.\n\n따라서, 에이전트의 주된 목표는 각 상태에 대해 최적의 행동을 선택함으로써, 할인된 총 보상을 최대화하는 정책을 개발하는 것입니다. 이 과정에서 다양한 강화학습 알고리즘이 사용될 수 있으며, 각각은 다른 방식으로 문제를 해결하고 최적의 솔루션을 찾습니다.\n","fileName":"2024-03-06","route":"2024-03-06"},"post":{"meta":{"title":"value function ( expected return maximize )","desc":"가치함수의 개념","date":"2024.03.08","tags":["AI","machine learning","Reinforce ment learning","value function"]},"content":"\n\n\n### Markov Decision Process (MDP)에서 기대 반환값 최대화를 위한 핵심 개념들\n\n#### 1. 정책(Policy)\n**정의**: 에이전트가 특정 상태에서 취할 수 있는 행동을 결정하는 규칙입니다.\n**유형**: 결정론적(항상 동일한 행동) 또는 확률적(각 행동의 선택 확률).\n  \n#### 2. 가치 함수(Value Function)\n**상태 가치 함수(State Value Function)(V(s))**: 정책 하에서 특정 상태에서 시작해 얻을 수 있는 기대 반환값입니다.\n![240421-135952](/posts/2024-03-08/240421-135952.png)\n**행동 가치 함수(Action Value Function)(Q(s, a))**: 특정 상태와 행동에서 시작해 정책 하에서 얻을 수 있는 기대 반환값입니다.\n![240421-140004](/posts/2024-03-08/240421-140004.png)\n\n#### 3. 벨만 방정식(Bellman Equation)\n**기능**: 가치 함수를 재귀적으로 계산하며, MDP의 동적 특성을 모델링합니다.\n**형태**: 현재 상태의 가치는 즉각적인 보상과 다음 상태들의 할인된 가치들의 합입니다.\n\n#### 4. 최적화 알고리즘(Optimization Algorithms)\n**동적 프로그래밍**: 가치 반복과 정책 반복을 포함하며, 계산적으로 효율적인 솔루션을 제공합니다.\n**몬테 카를로 방법**: 경험을 통해 가치 함수를 추정하며, 모델이 필요 없는 경우 유용합니다.\n**시간차 학습**: Q-Learning과 Sarsa 등을 포함하며, 직접 경험으로부터 가치 함수를 업데이트합니다.\n\n#### 5. 탐색과 활용(Exploration and Exploitation)\n**중요성**: 환경을 충분히 탐색하여 정보를 수집하고, 알려진 정보를 사용하여 최대 보상을 얻어야 합니다.\n**전략**: ε-greedy 정책과 같은 전략을 사용하여 적절한 균형을 유지합니다.\n\n### 결론\nMDP에서 기대 반환값의 최대화는 이러한 개념들을 통합하여 최적의 행동 전략을 학습하는 과정입니다. 각 개념은 에이전트가 효과적으로 학습하고, 환경 내에서 최대의 보상을 획득할 수 있도록 도와줍니다.\n","fileName":"2024-03-08","route":"2024-03-08"},"next":null},"__N_SSG":true}