{"pageProps":{"prev":{"meta":{"title":"github blog 구축기(4)","desc":"marked 라이브러리를 통한 markdown csr","date":"2024.02.07","tags":["next.js","github blog","gh-pages","mdx","marked","highlights"]},"content":"\n# mdx로 작성한 파일의 정제\n\n> mdx 작성한 meta 정보를 제거하고싶었다.\n> 하지만 정보 제거에 어려움을 ...ㅠㅠ\n> next.js 내부에서 모두 해결하고자 했으나, 일단 정적 페이지로 만든 페이지의 경우 SEO 적용도 가능하므로\n> 굳이 여기에 시간 쓰지않고, markdown 문법을 react로 표출할 수 있도록 해주는 라이브러리를 찾기로 결정~!\n\n![240208-121859](/posts/2024-02-07/240208-121859.png)\n그림1. markdown 표출에 많이 쓰이는 npm library ( NPM trends )\n\n가장 사용량이 많고, 최근 없데이트가 되는걸로 보이는 marked를 이용하기로 결심\n\n## marked library\n\n> 사용방법이 어렵지 않고, markdown 문법의 문자열을 집어넣어넣고, 그 결과를 innerHTML로 넣으면 간단하게 표출됨을 확인\n\n```tsx\n import { Marked } from 'marked';\n import { markedHighlight } from 'marked-highlight';\n import hljs from 'highlight.js';\n import 'highlight.js/styles/github.css';\n\n const marked = new Marked(\n   markedHighlight({\n     highlight(code, lang) {\n       const language = hljs.getLanguage(lang) ? lang : 'plaintext';\n       return hljs.highlight(code, { language }).value;\n     },\n   })\n );\n\n const renderer = new marked.Renderer();\n\n renderer.code = function (code: string) {\n   return '<div class=\"mockup-code border bg-[#f6f8fa]\" style=\"width:90%;margin:auto;margin-bottom:20px;\"><pre style=\"margin-bottom:0;padding:0;\"><code>\\n${marked.parseInline(code)}</code></pre></div>';\n };\n\n\n <div\n   className='markdown-body min-h-[58vh] px-6 max-w-[100vw]'\n   dangerouslySetInnerHTML={{\n     __html: marked.parse(post.content, {\n       renderer,\n     }),\n }}>\n```\n\ncode box 영역에 highlight를 적용하고 daisyUI에서 제공하는 mockup-code 를 활용하고 싶어 위와같이 짰다.\n![240208-122304](/posts/2024-02-07/240208-122304.png)\n그림2. daisyUI mockup code UI\n\n이렇게 하면 next.js에서 mdx로 작성한 페이지의 meta정보와 별개로 markdown 문법을 내 입맛에 맞게 표출 할 수 있게 되었다.\n","fileName":"2024-02-07","route":"2024-02-07"},"post":{"meta":{"title":"강화학습에 대한 계층적 이해","desc":"강화학습에 대한 학습","date":"2024.03.02","tags":["AI","machine learning","Reinforce ment learning"]},"content":"\n\n### 1단계: 기본 개념의 이해\n- **에이전트(Agent)**: 강화학습 시스템에서 학습과 행동을 수행하는 주체입니다.\n- **환경(Environment)**: 에이전트가 상호작용하는 외부 세계로, 에이전트의 행동에 반응하여 상태를 변화시키고 보상을 제공합니다.\n- **보상(Reward)**: 에이전트의 행동에 대한 즉각적인 피드백으로, 목표 달성에 얼마나 기여했는지를 나타냅니다.\n- **상태(State)**: 에이전트가 인식할 수 있는 환경의 현재 상황을 나타냅니다.\n\n### 2단계: 행동과 정책의 이해\n- **행동(Action)**: 에이전트가 상태에 따라 취할 수 있는 구체적인 조치입니다.\n- **정책(Policy)**: 주어진 상태에서 어떤 행동을 선택할지 결정하는 규칙 또는 전략입니다.\n\n### 3단계: 가치 함수와 Q-학습\n- **가치 함수(Value Function)**: 특정 정책 하에서 주어진 상태에서 시작하여 기대할 수 있는 미래 보상의 총합을 평가합니다.\n- **Q-함수(Q Function)**: 주어진 상태에서 특정 행동을 취했을 때 기대할 수 있는 미래 보상의 총합을 평가합니다.\n- **Q-러닝(Q-learning)**: 경험을 통해 Q-함수를 학습하는 방법으로, 모델 프리 강화학습 알고리즘 중 하나입니다.\n\n### 4단계: 심화 개념과 전략\n- **에피소드(Episode)**: 초기 상태에서 시작하여 종료 상태에 도달할 때까지의 일련의 행동, 상태, 보상의 시퀀스입니다.\n- **탐험과 활용(Exploration and Exploitation)**: 에이전트가 미지의 행동을 탐험하는 것과 최적의 행동을 활용하는 것 사이의 균형을 맞추는 전략입니다.\n- **감가율(Discount Factor)**: 미래의 보상을 현재 가치로 환산할 때 사용하는 계수로, 먼 미래의 보상을 현재보다 덜 중요하게 평가하는 데 사용됩니다.\n\n### 5단계: 고급 알고리즘과 최적화\n- **정책 그래디언트(Policy Gradient)**: 정책 자체를 직접 최적화하는 기법입니다.\n- **딥 강화학습(Deep Reinforcement Learning)**: 심층 신경망을 이용하여 복잡한 환경에서의 강화학습 문제를 해결하는 방법입니다.\n- **멀티 에이전트 강화학습(Multi-agent Reinforcement Learning)**: 여러 에이전트가 상호작용하는 환경에서의 학습 전략을 다룹니다.\n","fileName":"2024-03-02","route":"2024-03-02"},"next":{"meta":{"title":"Q-learning","desc":"Q-learning에 대한 학습","date":"2024.03.04","tags":["AI","machine learning","Reinforce ment learning","Q-learning"]},"content":"\n\n\n\n\n## **Q-learning에 대한 설명**\n\nQ-learning은 강화학습의 한 형태로, 에이전트가 어떤 정책(policy)에 의존하지 않고 최적의 행동을 학습할 수 있는 모델 프리(model-free) 알고리즘입니다. 이 알고리즘의 목표는 각 상태(state)에서 취할 수 있는 모든 행동(action)의 Q-값(quality)을 추정하는 것으로, Q-값은 특정 상태에서 특정 행동을 취했을 때 기대할 수 있는 미래 보상의 총합입니다. 에이전트는 이 Q-값을 기반으로 행동을 선택하며, 시간이 지남에 따라 최적의 행동 선택을 위해 Q-값을 지속적으로 업데이트합니다.\n\n## Q-learning의 계층적 중요 개념 설명\n\n**1단계: 기본 구성 요소**\n**상태(State)**: 에이전트가 인식하는 환경의 현재 상황입니다. 각 상태는 Q-테이블에서 하나의 행을 형성합니다.\n**행동(Action)**: 에이전트가 선택할 수 있는 각각의 선택지입니다. 각 행동은 Q-테이블에서 하나의 열을 형성합니다.\n\n**2단계: Q-값과 Q-테이블**\n**Q-값(Q-value)**: 주어진 상태에서 특정 행동을 취했을 때 얻을 수 있는 예상 미래 보상을 나타냅니다. Q-값은 Q-테이블에 저장되며, 알고리즘이 진행됨에 따라 업데이트됩니다.\n**Q-테이블(Q-table)**: 모든 상태와 행동 쌍에 대한 Q-값을 저장하는 테이블로, 학습 과정에서 지속적으로 업데이트됩니다.\n\n**3단계: 학습 과정**\n**보상(Reward)**: 특정 행동 후 환경으로부터 받는 피드백입니다. 보상은 Q-값의 업데이트에 중요한 역할을 합니다.\n**학습률(Learning Rate, α)**: Q-값을 업데이트할 때 이전의 Q-값과 새로운 정보의 가중치를 결정합니다.\n**감가율(Discount Factor, γ)**: 미래 보상의 현재 가치를 계산할 때 사용되는 계수로, 먼 미래의 보상을 현재보다 덜 중요하게 만듭니다.\n\n**4단계: 정책 결정**\n**탐욕적 선택(Greedy Action Selection)**: 항상 최대 Q-값을 가진 행동을 선택하는 방법입니다.\n**탐험(Exploration)**: 가끔은 최적이 아닌 행동을 선택하여 미지의 상태-행동 쌍에 대한 정보를 얻습니다. 탐험을 통해 최적화에 빠지는 것을 방지하고, 보다 효과적인 학습을 촉진합니다.\n\n**5단계: 알고리즘 최적화**\n**ε-탐욕적 알고리즘(ε-Greedy Algorithm)**: 대부분의 경우 최적의 행동을 선택하되, 일정 확률(ε)로 무작위 행동을 선택하여 탐험의 기회를 제공합니다.\n**학습의 수렴**: Q-값이 안정화되고 변화가 줄어들 때 학습이 수렴된 것으로 간주됩니다. 이는 최적의 행동 정책이 학습되었음을 의미합니다.\n\n\n\n### **ε-Greedy**\n\nε-Greedy 알고리즘은 강화학습에서 탐욕적인 접근법의 한계를 극복하고자 사용되는 전략입니다. 이 알고리즘은 항상 최고의 보상을 제공하는 행동을 선택하는 순수 탐욕적 방법과 달리, 일정 확률로 무작위 행동을 선택하여 탐험을 촉진합니다. 이는 초기에 잘못된 정보에 기반한 결정으로부터 벗어나 보다 다양한 경험을 통해 최적의 정책을 발견할 수 있게 합니다.\n\n### ε-Greedy 알고리즘의 효과\n\n**탐험 촉진**: ε-Greedy 알고리즘은 에이전트가 미지의 행동을 탐색할 수 있는 기회를 제공하여, 환경에 대한 보다 깊은 이해와 더 나은 결정을 가능하게 합니다.\n**극소화 문제 완화**: ε-Greedy는 에이전트가 초기에 발견한 '최적' 해결책에만 집착하는 것을 방지하고, 다양한 가능성을 탐색함으로써 더 나은 해결책을 찾을 수 있게 합니다.\n**정책의 수렴 개선**: 무작위 선택을 통해 에이전트는 다양한 상태와 행동에 대한 정보를 축적하며, 이는 최적의 정책으로의 수렴을 도와줍니다.\n**성능 최적화**: ε-Greedy는 장기적으로 에이전트의 성능을 최적화하며, 탐험과 활용의 균형을 통해 최적의 결과를 도출합니다.\n\n\n### ε-Greedy에서의 Decaying ε 개념\n\n**Decaying ε**는 ε-greedy 알고리즘에서 사용되는 전략으로, 에이전트의 학습 과정 동안 **ε(탐험률)** 의 값을 점차 감소시키는 방법입니다. 이 방법은 에이전트가 초기에는 다양한 행동을 많이 탐험하도록 하고, 학습이 진행됨에 따라 점차 활용을 증가시키는 것을 목표로 합니다.\n\n### Decaying ε의 필요성과 효과\n\n**초기 탐험의 중요성**: 학습 초기에 에이전트는 환경에 대한 정보가 부족하기 때문에 높은 탐험률을 통해 다양한 행동을 시도하고 필요한 데이터를 수집합니이는 에이전트가 보다 광범위한 경험을 통해 최적의 정책을 학습할 수 있도록 돕습니다.\n**점진적 활용 증가**: 에이전트가 더 많은 데이터와 경험을 축적하면서, 탐험의 필요성이 감소하고 학습된 정보를 활용하는 것이 더 효과적입니ε 값을 점차 줄임으로써 에이전트는 최적의 행동을 선택할 확률을 높이고, 불필요한 리스크를 줄일 수 있습니다.\n**성능 및 수렴 개선**: Decaying ε를 사용하면 에이전트는 학습 초기에는 탐험을 통해 최적의 정책을 찾아가고, 학습 후반에는 이 정책을 정제하고 확고히 하는 단계로 넘어갑니다. 이는 에이전트가 더 빠르고 효과적으로 최적의 정책에 수렴하도록 돕습니다.\n\n## 구현 방법\n\nDecaying ε 전략을 구현할 때는 ε의 초기 값이 상대적으로 높게 설정되고, 매 스텝이나 에피소드마다 ε 값을 일정 비율로 감소시키는 방법을 사용합니다. 이 감소율은 고정되거나 에이전트의 성능에 따라 조정될 수 있으며, 학습의 마지막 단계에서는 거의 순수한 활용 방식을 취하게 합니다.\n\nDecaying ε는 학습 과정에서 탐험과 활용의 균형을 잘 조절하면서 에이전트의 전반적인 성능을 최적화하는 데 매우 중요한 역할을 합니다.\n\n\n### Q-Learning에서의 감마(γ) 값의 역할\n\n**감마(γ)**, 또는 **할인 계수(Discount Factor)** 는 Q-Learning 알고리즘에서 미래의 보상을 현재 가치로 할인하는 데 사용되는 파라미터입니다. 이 값은 0과 1 사이에서 설정되며, 미래 보상의 현재 가치를 어떻게 평가할지 결정하는 중요한 요소입니다.\n\n### 감마(γ) 값의 중요성\n\n1. **미래 보상의 중요도 설정**: 감마 값이 높을수록 (1에 가까울수록) 에이전트는 미래의 보상을 현재와 거의 동등하게 평가합니다. 이는 장기적인 목표를 추구하는데 유리하며, 반대로 감마 값이 낮으면 (0에 가까우면) 에이전트는 즉각적인 보상을 더 중시하게 됩니다.\n2. **학습의 안정성**: 적절한 감마 값은 에이전트의 학습 과정을 안정화시키는 데 도움을 줍니다. 너무 높은 감마 값은 미래의 불확실한 보상으로 인해 학습이 불안정해질 수 있고, 너무 낮은 값은 장기적인 최적 전략을 놓칠 수 있습니다.\n3. **수렴의 향상**: 감마 값은 Q-값의 수렴 속도와 안정성에 직접적인 영향을 미칩니다. 올바르게 설정된 감마 값은 강화학습 모델의 성능과 효율성을 크게 향상시킬 수 있습니다.\n\n### 감마 값의 선택\n\n감마 값은 특정 환경과 에이전트의 목표에 맞게 조절될 수 있습니다. 장기적인 이익을 중시하는 경우 높은 감마 값을 설정하고, 단기적인 결과가 중요할 때는 낮은 감마 값을 사용하는 것이 일반적입니다.\n감마(γ) 값은 Q-Learning과 같은 강화학습 알고리즘에서 미래 보상의 현재 가치를 결정하는 중요한 요소입니다. 이 값은 에이전트의 행동과 학습 전략, 최종적인 학습 결과에 직접적인 영향을 미치며, 에이전트의 성능 최적화에 필수적인 역할을 합니다.\n\n\n\n### Q-Learning의 Q-Update 공식\n\nQ-Learning 알고리즘에서 **Q-Update**는 에이전트의 학습 과정에서 사용되는 핵심 메커니즘입니다. 이 과정은 각 에피소드에서 에이전트가 취한 행동의 결과로 받은 보상과 다음 상태에 대한 최대 Q-값을 기반으로 현재의 Q-값을 업데이트하는 것을 포함합니다.\n\n\n### Q-Update 공식의 구성\nQ-값 업데이트는 다음 공식을 사용하여 수행됩니다:\n![240421-132221](/posts/2024-03-04/240421-132221.png)\n\n여기서:\n![240421-132300](/posts/2024-03-04/240421-132300.png)\n\n### Q-Update의 역할과 중요성\n\n1. **정책 개선**: Q-Update 공식은 현재 정책을 꾸준히 개선하기 위해 설계되었습니다. 에이전트는 받은 보상과 가능한 최고의 미래 보상을 고려하여 자신의 행동을 조정합니다.\n2. **수렴 보장**: 적절한 조건 하에, Q-Learning은 최적 정책으로 수렴한다는 이론적 보장을 제공합니다. Q-Update는 이 수렴을 가능하게 하는 중심적인 메커니즘입니다.\n3. **가치 기반 결정**: Q-값은 특정 상태에서 특정 행동을 취할 때 기대할 수 있는 가치를 나타냅니다. 이 값을 업데이트하는 과정은 에이전트가 더 정보에 기반한 결정을 내리도록 돕습니다.\n\n### Q-Update의 구현\n\nQ-Update는 각 에피소드 또는 스텝에서 실행되며, 에이전트의 경험을 통해 점진적으로 학습합니다. 이 과정은 에이전트가 새로운 상태를 탐험하고, 다양한 상황에서 얻은 데이터로부터 학습을 진행하면서 점점 더 효과적인 행동을 선택하도록 유도합니다.\nQ-Update는 Q-Learning 알고리즘의 핵심으로, 에이전트가 환경과의 상호작용을 통해 최적의 행동을 학습하고 결정적인 개선을 달성할 수 있도록 돕습니다. 이 과정은 강화학습에서 중요한 역할을 수행하며, 효과적인 학습과 최적의 정책 발견을 위한 필수적인 접근 방법입니다.\n\n","fileName":"2024-03-04","route":"2024-03-04"}},"__N_SSG":true}