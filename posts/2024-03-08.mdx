---
title: value function ( expected return maximize )
desc: 가치함수의 개념
date: 2024.03.08
tags: [AI, machine learning, Reinforce ment learning, value function]
---


#### 1. 정책(Policy)
**정의**: 에이전트가 특정 상태에서 취할 수 있는 행동을 결정하는 규칙입니다.
**유형**: 결정론적(항상 동일한 행동) 또는 확률적(각 행동의 선택 확률).
  
#### 2. 가치 함수(Value Function)
**상태 가치 함수(State Value Function)(V(s))**: 정책 하에서 특정 상태에서 시작해 얻을 수 있는 기대 반환값입니다.
![240421-135952](/posts/2024-03-08/240421-135952.png)
**행동 가치 함수(Action Value Function)(Q(s, a))**: 특정 상태와 행동에서 시작해 정책 하에서 얻을 수 있는 기대 반환값입니다.
![240421-140004](/posts/2024-03-08/240421-140004.png)

#### 3. 벨만 방정식(Bellman Equation)
**기능**: 가치 함수를 재귀적으로 계산하며, MDP의 동적 특성을 모델링합니다.
**형태**: 현재 상태의 가치는 즉각적인 보상과 다음 상태들의 할인된 가치들의 합입니다.

#### 4. 최적화 알고리즘(Optimization Algorithms)
**동적 프로그래밍**: 가치 반복과 정책 반복을 포함하며, 계산적으로 효율적인 솔루션을 제공합니다.
**몬테 카를로 방법**: 경험을 통해 가치 함수를 추정하며, 모델이 필요 없는 경우 유용합니다.
**시간차 학습**: Q-Learning과 Sarsa 등을 포함하며, 직접 경험으로부터 가치 함수를 업데이트합니다.

#### 5. 탐색과 활용(Exploration and Exploitation)
**중요성**: 환경을 충분히 탐색하여 정보를 수집하고, 알려진 정보를 사용하여 최대 보상을 얻어야 합니다.
**전략**: ε-greedy 정책과 같은 전략을 사용하여 적절한 균형을 유지합니다.

### 결론
MDP에서 기대 반환값의 최대화는 이러한 개념들을 통합하여 최적의 행동 전략을 학습하는 과정입니다. 각 개념은 에이전트가 효과적으로 학습하고, 환경 내에서 최대의 보상을 획득할 수 있도록 도와줍니다.
