---
title: Markov Decision Process
desc: Markov Decision Process
date: 2024.03.06
tags: [AI, machine learning, Reinforce ment learning, Markov Decision Process, MDP]
---






### Markov Decision Process (MDP)의 정의

**마르코프 결정 과정(Markov Decision Process, MDP)** 은 강화학습의 수학적 모델 중 하나로, 순차적 의사결정 문제를 다루는 데 사용됩니다. MDP는 **상태(states)**, **행동(actions)**, **보상 함수(rewards)**, 그리고 **상태 전이 확률(transitions)** 이라는 네 가지 주요 구성 요소로 이루어져 있습니다.

### MDP의 구성 요소

**상태(S)**: 에이전트가 처할 수 있는 모든 가능한 환경의 상황을 의미합니다.
**행동(A)**: 에이전트가 각 상태에서 취할 수 있는 모든 가능한 행동을 나타냅니다.
**보상 함수(R)**: 특정 상태에서 특정 행동을 했을 때 에이전트가 받게 되는 보상을 정의합니다. 이 보상은 에이전트가 얼마나 "잘" 행동했는지를 수치적으로 평가하는 지표입니다.
**상태 전이 확률(P)**: 어떤 상태에서 특정 행동을 한 후 다른 상태로 이동할 확률을 나타냅니다. 이는 환경의 동적인 특성을 모델링하는 데 중요합니다.

### MDP의 특징

**마르코프 성질**: MDP의 기본 가정 중 하나는 마르코프 성질입니다. 이는 미래의 상태가 오직 현재의 상태와 행동에만 의존하며, 과거의 상태나 행동에는 의존하지 않는다는 성질을 의미합니다.
**결정론적 및 확률적 행동**: MDP에서는 행동이 결정론적이거나 확률적일 수 있습니다. 결정론적인 경우, 행동 결과는 항상 같은 상태로 이어지며, 확률적인 경우 다양한 가능성이 존재합니다.

### MDP의 적용

MDP는 다양한 실세계 문제에 적용할 수 있습니다. 예를 들어, 자율 주행 자동차, 로봇 팔 조작, 경제 정책 결정, 게임 플레이 등 복잡한 결정을 요구하는 다양한 분야에서 사용됩니다. 이러한 문제들을 모델링하고 최적의 결정 규칙(정책)을 학습하는 것이 MDP를 사용하는 주된 목적입니다.



### MDP의 목표
MDP는 강화학습에서 중요한 이론적 기반을 제공하며, 복잡한 의사결정 문제를 효과적으로 해결하기 위한 강력한 도구입니다. 이 모델을 통해 에이전트는 최적의 행동 전략을 배울 수 있으며, 이는 궁극적으로 **주어진 환경에서 최대의 보상**을 얻기 위한 방법을 찾는 데 도움을 줍니다.
마르코프 결정 과정(Markov Decision Process, MDP)의 주요 목표는 기대되는 반환값(보상의 합)을 최대화하는 것입니다. 이 목표를 달성하기 위해 에이전트는 상태와 행동 간의 상호작용을 통해 최적의 정책(policy)를 찾아내야 합니다. 최적의 정책은 주어진 상태에서 에이전트가 선택할 수 있는 행동들 중에서 기대 반환값을 최대화하는 행동을 선택하도록 합니다.

MDP에서 반환값은 일련의 행동을 통해 얻은 보상의 미래 가치를 고려한 총합입니다. 이 보상은 종종 감마(γ)라는 할인 계수를 사용하여 할인되며, 이는 미래의 보상이 현재 가치에 미치는 영향을 조정합니다. 할인 계수는 보상을 받는 시점의 지연에 따른 가치의 감소를 반영합니다.

따라서, 에이전트의 주된 목표는 각 상태에 대해 최적의 행동을 선택함으로써, 할인된 총 보상을 최대화하는 정책을 개발하는 것입니다. 이 과정에서 다양한 강화학습 알고리즘이 사용될 수 있으며, 각각은 다른 방식으로 문제를 해결하고 최적의 솔루션을 찾습니다.
