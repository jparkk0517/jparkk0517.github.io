<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jparkk0517.github.io"/><meta property="og:title" content="value function ( expected return maximize )"/><meta property="og:description" content="가치함수의 개념"/><meta name="next-head-count" content="6"/><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.5.0/github-markdown-light.min.css" integrity="sha512-Pmhg2i/F7+5+7SsdoUqKeH7UAZoVMYb1sxGOoJ0jWXAEHP0XV2H4CITyK267eHWp2jpj7rtqWNkmEOw1tNyYpg==" crossorigin="anonymous" referrerPolicy="no-referrer"/><link rel="preload" href="/_next/static/css/a3380dab395b114b.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/a3380dab395b114b.css" crossorigin="" data-n-g=""/><link rel="preload" href="/_next/static/css/5c3da54741072322.css" as="style" crossorigin=""/><link rel="stylesheet" href="/_next/static/css/5c3da54741072322.css" crossorigin="" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" crossorigin="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-c5490265a50f55a3.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/framework-550f72cae410482c.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/main-e6e0b507836a963c.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/_app-029e80c529502cd5.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/29-bed587a0543d4f84.js" defer="" crossorigin=""></script><script src="/_next/static/chunks/pages/posts/%5BpostId%5D-da2083a5df0f2bb7.js" defer="" crossorigin=""></script><script src="/_next/static/i8a_E8Pc4o84P8FH3frA6/_buildManifest.js" defer="" crossorigin=""></script><script src="/_next/static/i8a_E8Pc4o84P8FH3frA6/_ssgManifest.js" defer="" crossorigin=""></script></head><body><div id="__next"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K5DKF9QN" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div class="h-screen"><div class="navbar sticky top-0 z-10 bg-base-100"><div class="flex-1"><a class="btn btn-ghost text-xl">SunBlog</a></div><div class="flex-none gap-2"><div class="form-control"><div class="join"><div><div class="flex rounded-l-xl rounded-r-none border-2 border-r-0 border-base-300"><input class="input join-item input-sm h-[30px] max-w-[30vw] focus:border-none focus:outline-none" placeholder="" value=""/><span class="mr-2 cursor-pointer"><svg class="h-full w-4 text-gray-600" fill="none" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" viewBox="0 0 24 24" stroke="currentColor"><path d="M6 18L18 6M6 6l12 12"></path></svg></span></div></div><select class="join-item select select-bordered select-sm h-[34px] border-2 focus:outline-none"><option value="tag">키워드</option><option value="title" selected="">제목</option></select><div class="indicator"><button class="btn join-item btn-sm h-[34px]">조회</button></div></div></div></div></div><main class="prose h-[95vh] overflow-y-scroll p-4 align-baseline"><div class="mb-20"><div class="hero bg-base-300"><div class="hero-content text-center"><div class="max-w-md"><h1 class="text-5xl font-bold">value function ( expected return maximize )</h1><p class="my-4">2024.03.08</p><div class="mb-4 max-w-[80vw] break-words">가치함수의 개념</div><div class="badge badge-outline mr-1">AI</div><div class="badge badge-outline mr-1">machine learning</div><div class="badge badge-outline mr-1">Reinforce ment learning</div><div class="badge badge-outline mr-1">value function</div></div></div></div><div class="divider"></div><div class="min-h-[58vh] max-w-[100vw] whitespace-pre-wrap bg-white px-6"><h3 class="border-b-2 font-bold text-5xl mb-5 text-gray-500">Markov Decision Process (MDP)에서 기대 반환값 최대화를 위한 핵심 개념들</h3><h4 class="border-b-2 font-bold text-5xl mb-5 text-gray-500">1. 정책(Policy)</h4><p><strong>정의</strong>: 에이전트가 특정 상태에서 취할 수 있는 행동을 결정하는 규칙입니다.
<strong>유형</strong>: 결정론적(항상 동일한 행동) 또는 확률적(각 행동의 선택 확률).</p>
<h4 class="border-b-2 font-bold text-5xl mb-5 text-gray-500">2. 가치 함수(Value Function)</h4><p><strong>상태 가치 함수(State Value Function)(V(s))</strong>: 정책 하에서 특정 상태에서 시작해 얻을 수 있는 기대 반환값입니다.
<img src="/posts/2024-03-08/240421-135952.png" alt="240421-135952">
<strong>행동 가치 함수(Action Value Function)(Q(s, a))</strong>: 특정 상태와 행동에서 시작해 정책 하에서 얻을 수 있는 기대 반환값입니다.
<img src="/posts/2024-03-08/240421-140004.png" alt="240421-140004"></p>
<h4 class="border-b-2 font-bold text-5xl mb-5 text-gray-500">3. 벨만 방정식(Bellman Equation)</h4><p><strong>기능</strong>: 가치 함수를 재귀적으로 계산하며, MDP의 동적 특성을 모델링합니다.
<strong>형태</strong>: 현재 상태의 가치는 즉각적인 보상과 다음 상태들의 할인된 가치들의 합입니다.</p>
<h4 class="border-b-2 font-bold text-5xl mb-5 text-gray-500">4. 최적화 알고리즘(Optimization Algorithms)</h4><p><strong>동적 프로그래밍</strong>: 가치 반복과 정책 반복을 포함하며, 계산적으로 효율적인 솔루션을 제공합니다.
<strong>몬테 카를로 방법</strong>: 경험을 통해 가치 함수를 추정하며, 모델이 필요 없는 경우 유용합니다.
<strong>시간차 학습</strong>: Q-Learning과 Sarsa 등을 포함하며, 직접 경험으로부터 가치 함수를 업데이트합니다.</p>
<h4 class="border-b-2 font-bold text-5xl mb-5 text-gray-500">5. 탐색과 활용(Exploration and Exploitation)</h4><p><strong>중요성</strong>: 환경을 충분히 탐색하여 정보를 수집하고, 알려진 정보를 사용하여 최대 보상을 얻어야 합니다.
<strong>전략</strong>: ε-greedy 정책과 같은 전략을 사용하여 적절한 균형을 유지합니다.</p>
<h3 class="border-b-2 font-bold text-5xl mb-5 text-gray-500">결론</h3><p>MDP에서 기대 반환값의 최대화는 이러한 개념들을 통합하여 최적의 행동 전략을 학습하는 과정입니다. 각 개념은 에이전트가 효과적으로 학습하고, 환경 내에서 최대의 보상을 획득할 수 있도록 도와줍니다.</p>
</div><div class="mt-10 flex max-w-[100vw] justify-between"><button class="btn gap-2 md:btn-md lg:gap-3  w-[48%]"><svg class="size-6 fill-current md:size-8" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M15.41,16.58L10.83,12L15.41,7.41L14,6L8,12L14,18L15.41,16.58Z"></path></svg><span class="w-[70%] overflow-hidden text-xs">Markov Decision Process</span></button><button class="btn gap-2 md:btn-md lg:gap-3 btn-disabled w-[48%]"><span class="w-[70%] overflow-hidden  text-xs"></span><svg class="size-6 fill-current md:size-8" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z"></path></svg></button></div></div><footer class="footer footer-center bg-base-100 p-4 text-base-content"><aside><p>Copyright © 2024 - sun</p></aside></footer><div class="drawer absolute bottom-[5vh] left-5 z-30 w-0"><input id="my-drawer" type="checkbox" class="drawer-toggle"/><div class="drawer-content"><label class="btn btn-circle swap swap-rotate" for="my-drawer"><input type="checkbox"/><svg class="swap-off fill-current" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 512 512"><path d="M64,384H448V341.33H64Zm0-106.67H448V234.67H64ZM64,128v42.67H448V128Z"></path></svg><svg class="swap-on fill-current" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 512 512"><polygon points="400 145.49 366.51 112 256 222.51 145.49 112 112 145.49 222.51 256 112 366.51 145.49 400 256 289.49 366.51 400 400 366.51 289.49 256 400 145.49"></polygon></svg></label></div><div class="drawer-side"><label for="my-drawer" aria-label="close sidebar" class="drawer-overlay"></label><ul class="w-50 menu min-h-full bg-base-200 p-4 text-base-content"><li><a>Sun, 누구냐 너</a></li></ul></div></div></main></div></div><script id="__NEXT_DATA__" type="application/json" crossorigin="">{"props":{"pageProps":{"prev":{"meta":{"title":"Markov Decision Process","desc":"Markov Decision Process","date":"2024.03.06","tags":["AI","machine learning","Reinforce ment learning","Markov Decision Process","MDP"]},"content":"\n\n\n\n\n\n### Markov Decision Process (MDP)의 정의\n\n**마르코프 결정 과정(Markov Decision Process, MDP)** 은 강화학습의 수학적 모델 중 하나로, 순차적 의사결정 문제를 다루는 데 사용됩니다. MDP는 **상태(states)**, **행동(actions)**, **보상 함수(rewards)**, 그리고 **상태 전이 확률(transitions)** 이라는 네 가지 주요 구성 요소로 이루어져 있습니다.\n\n### MDP의 구성 요소\n\n**상태(S)**: 에이전트가 처할 수 있는 모든 가능한 환경의 상황을 의미합니다.\n**행동(A)**: 에이전트가 각 상태에서 취할 수 있는 모든 가능한 행동을 나타냅니다.\n**보상 함수(R)**: 특정 상태에서 특정 행동을 했을 때 에이전트가 받게 되는 보상을 정의합니다. 이 보상은 에이전트가 얼마나 \"잘\" 행동했는지를 수치적으로 평가하는 지표입니다.\n**상태 전이 확률(P)**: 어떤 상태에서 특정 행동을 한 후 다른 상태로 이동할 확률을 나타냅니다. 이는 환경의 동적인 특성을 모델링하는 데 중요합니다.\n\n### MDP의 특징\n\n**마르코프 성질**: MDP의 기본 가정 중 하나는 마르코프 성질입니다. 이는 미래의 상태가 오직 현재의 상태와 행동에만 의존하며, 과거의 상태나 행동에는 의존하지 않는다는 성질을 의미합니다.\n**결정론적 및 확률적 행동**: MDP에서는 행동이 결정론적이거나 확률적일 수 있습니다. 결정론적인 경우, 행동 결과는 항상 같은 상태로 이어지며, 확률적인 경우 다양한 가능성이 존재합니다.\n\n### MDP의 적용\n\nMDP는 다양한 실세계 문제에 적용할 수 있습니다. 예를 들어, 자율 주행 자동차, 로봇 팔 조작, 경제 정책 결정, 게임 플레이 등 복잡한 결정을 요구하는 다양한 분야에서 사용됩니다. 이러한 문제들을 모델링하고 최적의 결정 규칙(정책)을 학습하는 것이 MDP를 사용하는 주된 목적입니다.\n\n\n\n### MDP의 목표\nMDP는 강화학습에서 중요한 이론적 기반을 제공하며, 복잡한 의사결정 문제를 효과적으로 해결하기 위한 강력한 도구입니다. 이 모델을 통해 에이전트는 최적의 행동 전략을 배울 수 있으며, 이는 궁극적으로 **주어진 환경에서 최대의 보상**을 얻기 위한 방법을 찾는 데 도움을 줍니다.\n마르코프 결정 과정(Markov Decision Process, MDP)의 주요 목표는 기대되는 반환값(보상의 합)을 최대화하는 것입니다. 이 목표를 달성하기 위해 에이전트는 상태와 행동 간의 상호작용을 통해 최적의 정책(policy)를 찾아내야 합니다. 최적의 정책은 주어진 상태에서 에이전트가 선택할 수 있는 행동들 중에서 기대 반환값을 최대화하는 행동을 선택하도록 합니다.\n\nMDP에서 반환값은 일련의 행동을 통해 얻은 보상의 미래 가치를 고려한 총합입니다. 이 보상은 종종 감마(γ)라는 할인 계수를 사용하여 할인되며, 이는 미래의 보상이 현재 가치에 미치는 영향을 조정합니다. 할인 계수는 보상을 받는 시점의 지연에 따른 가치의 감소를 반영합니다.\n\n따라서, 에이전트의 주된 목표는 각 상태에 대해 최적의 행동을 선택함으로써, 할인된 총 보상을 최대화하는 정책을 개발하는 것입니다. 이 과정에서 다양한 강화학습 알고리즘이 사용될 수 있으며, 각각은 다른 방식으로 문제를 해결하고 최적의 솔루션을 찾습니다.\n","fileName":"2024-03-06","route":"2024-03-06"},"post":{"meta":{"title":"value function ( expected return maximize )","desc":"가치함수의 개념","date":"2024.03.08","tags":["AI","machine learning","Reinforce ment learning","value function"]},"content":"\n\n\n### Markov Decision Process (MDP)에서 기대 반환값 최대화를 위한 핵심 개념들\n\n#### 1. 정책(Policy)\n**정의**: 에이전트가 특정 상태에서 취할 수 있는 행동을 결정하는 규칙입니다.\n**유형**: 결정론적(항상 동일한 행동) 또는 확률적(각 행동의 선택 확률).\n  \n#### 2. 가치 함수(Value Function)\n**상태 가치 함수(State Value Function)(V(s))**: 정책 하에서 특정 상태에서 시작해 얻을 수 있는 기대 반환값입니다.\n![240421-135952](/posts/2024-03-08/240421-135952.png)\n**행동 가치 함수(Action Value Function)(Q(s, a))**: 특정 상태와 행동에서 시작해 정책 하에서 얻을 수 있는 기대 반환값입니다.\n![240421-140004](/posts/2024-03-08/240421-140004.png)\n\n#### 3. 벨만 방정식(Bellman Equation)\n**기능**: 가치 함수를 재귀적으로 계산하며, MDP의 동적 특성을 모델링합니다.\n**형태**: 현재 상태의 가치는 즉각적인 보상과 다음 상태들의 할인된 가치들의 합입니다.\n\n#### 4. 최적화 알고리즘(Optimization Algorithms)\n**동적 프로그래밍**: 가치 반복과 정책 반복을 포함하며, 계산적으로 효율적인 솔루션을 제공합니다.\n**몬테 카를로 방법**: 경험을 통해 가치 함수를 추정하며, 모델이 필요 없는 경우 유용합니다.\n**시간차 학습**: Q-Learning과 Sarsa 등을 포함하며, 직접 경험으로부터 가치 함수를 업데이트합니다.\n\n#### 5. 탐색과 활용(Exploration and Exploitation)\n**중요성**: 환경을 충분히 탐색하여 정보를 수집하고, 알려진 정보를 사용하여 최대 보상을 얻어야 합니다.\n**전략**: ε-greedy 정책과 같은 전략을 사용하여 적절한 균형을 유지합니다.\n\n### 결론\nMDP에서 기대 반환값의 최대화는 이러한 개념들을 통합하여 최적의 행동 전략을 학습하는 과정입니다. 각 개념은 에이전트가 효과적으로 학습하고, 환경 내에서 최대의 보상을 획득할 수 있도록 도와줍니다.\n","fileName":"2024-03-08","route":"2024-03-08"},"next":null},"__N_SSG":true},"page":"/posts/[postId]","query":{"postId":"2024-03-08"},"buildId":"i8a_E8Pc4o84P8FH3frA6","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>